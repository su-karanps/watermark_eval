{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94bbf1bc-7373-480f-8cf5-e981503dd231",
   "metadata": {},
   "source": [
    "# LLM Watermark Judger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf9eb3",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Load necessary packages and initialize OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cd394d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----- SYSTEM PACKAGES ----- ##\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "## ----- IMPORT FUNCTIONS ----- ##\n",
    "sys.path.insert(0, os.getcwd())\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa698589-4266-4ea0-9347-d36d73b6e2b3",
   "metadata": {},
   "source": [
    "## Judge New Samples\n",
    "\n",
    "Load sample pairs from a JSON file and use the GPT-judger to evaluate each sample, saving the results to the same data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "541ca5f2-221b-446b-a1c7-17c39d9aac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = load_from_json(filename=\"results.json\")\n",
    "\n",
    "for i in range(len(data_list)):\n",
    "\n",
    "    item = data_list[i]\n",
    "\n",
    "    # unpack needed fields\n",
    "    prompt = item[\"prompt\"]\n",
    "    uw_output = data_list[i][\"uw_output\"]\n",
    "    w_output = item[\"w_output\"]\n",
    "\n",
    "    # evaluate responses using gpt\n",
    "    judge_choice = \"\"\n",
    "    exception_counter = 0\n",
    "    while judge_choice == \"\" or judge_choice == \"Model Failure\":\n",
    "        try:\n",
    "            t1 = time.time()\n",
    "            judge_output, judge_choice, scores, is_randomized = gpt_judge(prompt, uw_output, w_output)\n",
    "            t2 = time.time()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "    # # save results\n",
    "    item[\"judge_output\"] = judge_output\n",
    "    item[\"randomized\"] = is_randomized\n",
    "    item[\"judge_choice\"] = judge_choice\n",
    "    item[\"scores\"] = scores\n",
    "\n",
    "    print(i+1, \"out of\", len(data_list), \"items processed!\", \"Execution Time:\", round(t2 - t1, 3), end=\"\\r\")\n",
    "    \n",
    "save_to_json(data_list, filename=\"results_judged.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ce611b-d8a9-4393-a9d8-66fdfdb3e299",
   "metadata": {},
   "source": [
    "## Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9c18714-1af9-469d-b909-d30ced7aeadd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# whether to display results as counts (0) or percentages (1)\n",
    "percent = 1\n",
    "\n",
    "# print results\n",
    "print(\"Judger Preference:\", extract_and_count_choices(data_list, 'judge_choice')[percent], \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
