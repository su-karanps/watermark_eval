{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94bbf1bc-7373-480f-8cf5-e981503dd231",
   "metadata": {},
   "source": [
    "# LLM Watermark Judger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf9eb3",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Load necessary packages and initialize OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cd394d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----- SYSTEM PACKAGES ----- ##\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9091a02f-26dd-4b82-85aa-862af650a7c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=\"[FILL IN]\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c8c1f5-3d3e-46f4-941b-b9b63d49023f",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c4feb7b-18dd-4ec7-bd70-2363a70bbc9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gpt-judger prompt and score extraction\n",
    "def gpt_judge(prompt, uw_output, w_output, is_randomized = None):\n",
    "\n",
    "    if is_randomized is None:\n",
    "            is_randomized = random.choice([True, False])\n",
    "    \n",
    "    # randomizing the order\n",
    "    a_output, b_output = (uw_output, w_output) if is_randomized else (w_output, uw_output)\n",
    "    \n",
    "    judge_prompt = \"\"\"[System]\n",
    "    Please act as an impartial judge and evaluate the quality of the text completions provided by two large language models to the prompt displayed below. \n",
    "    Assess each response according to the criteria outlined, using a 1-5 Likert scale where 1 indicates strong disagreement or the lowest quality, and 5 indicates strong agreement or the highest quality. \n",
    "    After scoring each criterion, provide a short summary for each response, including specific examples that influenced your scoring. \n",
    "    Additionally, don't let the length of the responses influence your evaluation. \n",
    "    Be as objective as possible and ensure that the order in which the responses are presented does not affect your decision.\n",
    "    \n",
    "    Criteria:\n",
    "    1. Relevance to the prompt\n",
    "    2. Depth of detail\n",
    "    3. Clarity of writing\n",
    "    4. Coherence and logical flow\n",
    "    5. Originality and insight\n",
    "    6. Use of specific examples\n",
    "    7. Accuracy of information\n",
    "    \n",
    "    Start with a brief statement about which response you think is superior. Then, for each response and criterion, provide a score, followed by a brief justification for that score. At the very end of your response, declare your verdict by choosing one of the choices below, strictly following the given format:\n",
    "    \n",
    "    \"[[A]]: [list of scores]\" if assistant A is better, \n",
    "    \"[[B]]: [list of scores]\" if assistant B is better, or \n",
    "    \"[[C]]\" for a tie.\n",
    "\n",
    "    [Prompt]\n",
    "    {prompt}\n",
    "    \n",
    "    [The Start of LLM A’s Answer]\n",
    "    {a_output}\n",
    "    [The End of LLM A’s Answer]\n",
    "    \n",
    "    [The Start of LLM B’s Answer]\n",
    "    {b_output}\n",
    "    [The End of LLM B’s Answer]\n",
    "    \"\"\".format(prompt=prompt, a_output=a_output, b_output=b_output)\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "        messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": judge_prompt,}],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "\n",
    "    judge_output = completion.choices[0].message.content\n",
    "\n",
    "    # search for a tie first\n",
    "    tie_pattern = r'\\[\\[C\\]\\]'\n",
    "    tie_match = re.search(tie_pattern, judge_output)\n",
    "    \n",
    "    if tie_match:\n",
    "        judge_choice = \"C\"\n",
    "        final_verdict = \"Tie\"\n",
    "        scores = []\n",
    "    else:\n",
    "        # pattern to match the verdict and the scores for A or B\n",
    "        pattern = r'\\[\\[([AB])\\]\\]: (?:\\[)?([5, 4, 3, 2, 1, ]+)(?:\\])?'\n",
    "        matches = re.findall(pattern, judge_output)\n",
    "        \n",
    "        if matches:\n",
    "            \n",
    "            # extract the last match which will have the choice and the corresponding scores\n",
    "            judge_choice, scores_str = matches[-1]\n",
    "            \n",
    "            # remove square brackets if they exist, strip whitespace, and split by comma\n",
    "            scores_str = scores_str.replace('[', '').replace(']', '').strip()\n",
    "            scores = [float(score) for score in scores_str.split(',')]\n",
    "            \n",
    "            # determine verdict based on the judge choice\n",
    "            if is_randomized:\n",
    "                final_verdict = \"Unwatermarked\" if judge_choice == 'A' else \"Watermarked\"\n",
    "            else:\n",
    "                final_verdict = \"Watermarked\" if judge_choice == 'A' else \"Unwatermarked\"\n",
    "        else:\n",
    "            final_verdict = \"Model Failure\"\n",
    "            scores = []\n",
    "    \n",
    "    return judge_output, final_verdict, scores, is_randomized\n",
    "\n",
    "# helper function to save results to a JSON file\n",
    "def save_to_json(data_list, filename=\"output.json\"):\n",
    "    with open(filename, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data_list, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# helper function to load results from a JSON file\n",
    "def load_from_json(filename=\"output.json\"):\n",
    "    with open(filename, 'r', encoding='utf-8') as json_file:\n",
    "        return json.load(json_file)\n",
    "    \n",
    "# function to tally judger results\n",
    "def extract_and_count_choices(data_list, category):\n",
    "    \n",
    "    # extract all 'category' entries from the list of dictionaries\n",
    "    choices = [entry[category] for entry in data_list]\n",
    "    \n",
    "    # count occurrences of each 'judge_choice'\n",
    "    count = {}\n",
    "    for choice in choices:\n",
    "        if choice in count:\n",
    "            count[choice] += 1\n",
    "        else:\n",
    "            count[choice] = 1\n",
    "\n",
    "    count_norm = count.copy()\n",
    "    for choice in count_norm:\n",
    "        count_norm[choice] /= len(choices)\n",
    "        count_norm[choice] *= 100\n",
    "        count_norm[choice] = np.round(count_norm[choice], 3)\n",
    "        \n",
    "    return count, count_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa698589-4266-4ea0-9347-d36d73b6e2b3",
   "metadata": {},
   "source": [
    "## Judge New Samples\n",
    "\n",
    "Load sample pairs from a JSON file and use the GPT-judger to evaluate each sample, saving the results to the same data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541ca5f2-221b-446b-a1c7-17c39d9aac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = load_from_json(filename=\"results.json\")\n",
    "\n",
    "for i in range(len(data_list)):\n",
    "\n",
    "    item = data_list[i]\n",
    "\n",
    "    # unpack needed fields\n",
    "    prompt = item[\"prompt\"]\n",
    "    uw_output = data_list[i][\"uw_output\"]\n",
    "    w_output = item[\"w_output\"]\n",
    "\n",
    "    # evaluate responses using gpt\n",
    "    judge_choice = \"\"\n",
    "    exception_counter = 0\n",
    "    while judge_choice == \"\" or judge_choice == \"Model Failure\":\n",
    "        try:\n",
    "            t1 = time.time()\n",
    "            judge_output, judge_choice, scores, is_randomized = gpt_judge(prompt, uw_output, w_output)\n",
    "            t2 = time.time()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "    # # save results\n",
    "    item[\"judge_output\"] = judge_output\n",
    "    item[\"randomized\"] = is_randomized\n",
    "    item[\"judge_choice\"] = judge_choice\n",
    "    item[\"scores\"] = scores\n",
    "\n",
    "    print(i+1, \"out of\", len(data_list), \"items processed!\", \"Execution Time:\", round(t2 - t1, 3), end=\"\\r\")\n",
    "    \n",
    "save_to_json(data_list, filename=\"results_judged.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ce611b-d8a9-4393-a9d8-66fdfdb3e299",
   "metadata": {},
   "source": [
    "## Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9c18714-1af9-469d-b909-d30ced7aeadd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judger Preference: {'Unwatermarked': 74.502, 'Watermarked': 18.327, 'Tie': 7.171} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# whether to display results as counts (0) or percentages (1)\n",
    "percent = 1\n",
    "\n",
    "# print results\n",
    "print(\"Judger Preference:\", extract_and_count_choices(data_list, 'judge_choice')[percent], \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
